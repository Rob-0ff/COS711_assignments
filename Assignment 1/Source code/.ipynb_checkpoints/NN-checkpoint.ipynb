{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robof\\AppData\\Local\\Temp\\ipykernel_30480\\2672841116.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column_to_average].mean(axis=1), inplace=True)\n",
      "C:\\Users\\robof\\AppData\\Local\\Temp\\ipykernel_30480\\2672841116.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column_to_average].mean(axis=1), inplace=True)\n",
      "C:\\Users\\robof\\AppData\\Local\\Temp\\ipykernel_30480\\2672841116.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column_to_average].mean(axis=1), inplace=True)\n",
      "C:\\Users\\robof\\AppData\\Local\\Temp\\ipykernel_30480\\2672841116.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column_to_average].mean(axis=1), inplace=True)\n",
      "C:\\Users\\robof\\AppData\\Local\\Temp\\ipykernel_30480\\2672841116.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column_to_average].mean(axis=1), inplace=True)\n",
      "C:\\Users\\robof\\AppData\\Local\\Temp\\ipykernel_30480\\2672841116.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column_to_average].mean(axis=1), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Type\n",
       "2    943\n",
       "1    933\n",
       "0    927\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform preprocessing\n",
    "\n",
    "data = perform_preprocessing() # type: ignore\n",
    "data['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, seed, in_features=12, num_hidden_layers=4, out_categories=3):\n",
    "        super(NNModel, self).__init__()\n",
    "        self.num_hidden = num_hidden_layers\n",
    "        hidden_layers = []\n",
    "        for i in range(0,num_hidden_layers):\n",
    "            hidden_layers.append(12)\n",
    "\n",
    "        self.input = nn.Linear(in_features, hidden_layers[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(hidden_layers[0], hidden_layers[1]))\n",
    "        for i in range(1, num_hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "        self.output = nn.Linear(12, out_categories)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = F.relu(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "learning_rate = 0.05\n",
    "epochs = 5000\n",
    "seed = round(time.time())\n",
    "num_hidden_layers = 2\n",
    "test_proportion = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(labels=['Type'], axis=1)\n",
    "y = data['Type']\n",
    "\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_proportion, random_state = seed)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train_model(optimiser, criterion, epochs, classifier):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        y_pred = classifier.forward(X_train)\n",
    "\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        losses.append(loss.detach().numpy())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch: {i} and loss: {loss}')\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    plt.plot(range(epochs), losses)\n",
    "    plt.ylabel(\"loss/error\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "    # See the model's prediction accuracy using testing set\n",
    "    with torch.no_grad():\n",
    "        y_val = classifier.forward(X_train)\n",
    "        loss = criterion(y_val, y_train)\n",
    "        print(f'Loss: {loss}')\n",
    "\n",
    "        correct = (torch.argmax(y_val, dim=1) == y_train).float()\n",
    "        accuracy = correct.sum() / len(correct)\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "    # Test the model with testing set after training model\n",
    "    with torch.no_grad():\n",
    "        y_val = classifier.forward(X_test)\n",
    "        loss = criterion(y_val, y_test)\n",
    "        print(f'Loss: {loss}')\n",
    "\n",
    "        correct = (torch.argmax(y_val, dim=1) == y_test).float()\n",
    "        accuracy = correct.sum() / len(correct)\n",
    "        print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPROP_run():\n",
    "  classifier = NNModel(seed=seed, num_hidden_layers=num_hidden_layers)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimiser = torch.optim.Rprop(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_model(optimiser, criterion, epochs, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_run():\n",
    "  classifier = NNModel(seed=seed, num_hidden_layers=num_hidden_layers)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimiser = torch.optim.SGD(classifier.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "  train_model(optimiser, criterion, epochs, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for num_hidden_layers: 2 and learning_rate: 0.03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m   plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m     71\u001b[0m   plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 73\u001b[0m averageLossResults, averageLossResults2, avergaeAccuracyResults, averageAccuracyResults2 \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m create_heatmap(averageLossResults, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Loss for RPROP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m create_heatmap(averageLossResults2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Loss for SGD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 37\u001b[0m, in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     36\u001b[0m     loss_2\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 37\u001b[0m     \u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     optimiser_2\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\rprop.py:104\u001b[0m, in \u001b[0;36mRprop.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    100\u001b[0m     maximize \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    102\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params, grads, prevs, step_sizes)\n\u001b[1;32m--> 104\u001b[0m     \u001b[43mrprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprevs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43metaminus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaminus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43metaplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaplus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\rprop.py:205\u001b[0m, in \u001b[0;36mrprop\u001b[1;34m(params, grads, prevs, step_sizes, foreach, maximize, differentiable, has_complex, step_size_min, step_size_max, etaminus, etaplus)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_rprop\n\u001b[1;32m--> 205\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprevs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43metaminus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaminus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43metaplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaplus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\rprop.py:251\u001b[0m, in \u001b[0;36m_single_tensor_rprop\u001b[1;34m(params, grads, prevs, step_sizes, step_size_min, step_size_max, etaminus, etaplus, maximize, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    249\u001b[0m     sign \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mmul(prev)\u001b[38;5;241m.\u001b[39msign()\n\u001b[0;32m    250\u001b[0m sign[sign\u001b[38;5;241m.\u001b[39mgt(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m=\u001b[39m etaplus\n\u001b[1;32m--> 251\u001b[0m sign[\u001b[43msign\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m etaminus\n\u001b[0;32m    252\u001b[0m sign[sign\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# update stepsizes with step size updates\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_hidden_layers_array = [2, 4, 6, 8, 10, 12, 14]\n",
    "learning_rates_array = [0.03, 0.06, 0.09, 0.12, 0.15, 0.18, 0.21]\n",
    "\n",
    "num_runs = 10\n",
    "\n",
    "def run():\n",
    "  finalLoss = np.zeros((len(num_hidden_layers_array), len(learning_rates_array)))\n",
    "  finalLoss_2 = np.zeros((len(num_hidden_layers_array), len(learning_rates_array)))\n",
    "  finalAccuracy = np.zeros((len(num_hidden_layers_array), len(learning_rates_array)))\n",
    "  finalAccuracy_2 = np.zeros((len(num_hidden_layers_array), len(learning_rates_array)))\n",
    "\n",
    "  for run in range(num_runs):\n",
    "      seed = round(time.time())\n",
    "      for i, num_hidden in enumerate(num_hidden_layers_array):\n",
    "          for j, learning_rate in enumerate(learning_rates_array):\n",
    "              print(f\"Running for num_hidden_layers: {num_hidden} and learning_rate: {learning_rate}\")\n",
    "              classifier = NNModel(seed=seed, num_hidden_layers=num_hidden)\n",
    "              classifier_2 = NNModel(seed=seed, num_hidden_layers=num_hidden) \n",
    "\n",
    "              criterion = nn.CrossEntropyLoss()\n",
    "              criterion_2 = nn.CrossEntropyLoss()\n",
    "\n",
    "              optimiser = torch.optim.Rprop(classifier.parameters(), lr=learning_rate)\n",
    "              optimiser_2 = torch.optim.SGD(classifier_2.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "              for epoch in range(epochs):\n",
    "                  y_pred = classifier.forward(X_train)\n",
    "                  y_pred_2 = classifier_2.forward(X_train)\n",
    "\n",
    "                  loss = criterion(y_pred, y_train)\n",
    "                  loss_2 = criterion_2(y_pred_2, y_train)\n",
    "\n",
    "                  optimiser.zero_grad()\n",
    "                  optimiser_2.zero_grad()\n",
    "                  loss.backward()\n",
    "                  loss_2.backward()\n",
    "                  optimiser.step()\n",
    "                  optimiser_2.step()\n",
    "\n",
    "              with torch.no_grad():\n",
    "                  y_val = classifier.forward(X_test)\n",
    "                  y_val_2 = classifier_2.forward(X_test)\n",
    "\n",
    "                  # Convert PyTorch tensors to scalars with .item() before adding to NumPy arrays\n",
    "                  finalLoss[i, j] += criterion(y_val, y_test).item()\n",
    "                  finalLoss_2[i, j] += criterion_2(y_val_2, y_test).item()\n",
    "\n",
    "                  correct = (torch.argmax(y_val, dim=1) == y_test).float()\n",
    "                  correct_2 = (torch.argmax(y_val_2, dim=1) == y_test).float()\n",
    "\n",
    "                  finalAccuracy[i, j] += correct.sum().item() / len(correct)\n",
    "                  finalAccuracy_2[i, j] += correct_2.sum().item() / len(correct_2)\n",
    "\n",
    "  # Averaging across the runs\n",
    "  finalLoss /= num_runs\n",
    "  finalLoss_2 /= num_runs\n",
    "  finalAccuracy /= num_runs\n",
    "  finalAccuracy_2 /= num_runs\n",
    "\n",
    "  return finalLoss, finalLoss_2, finalAccuracy, finalAccuracy_2\n",
    "\n",
    "\n",
    "def create_heatmap(results, heading):\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  sns.heatmap(results, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "  plt.xticks(ticks=np.arange(len(num_hidden_layers_array)), labels=num_hidden_layers_array)\n",
    "  plt.yticks(ticks=np.arange(len(learning_rates_array)), labels=learning_rates_array)\n",
    "  plt.gca().invert_yaxis()\n",
    "  plt.title(heading, fontsize=12)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "averageLossResults, averageLossResults2, avergaeAccuracyResults, averageAccuracyResults2 = run()\n",
    "\n",
    "create_heatmap(averageLossResults, \"Average Loss for RPROP\")\n",
    "create_heatmap(averageLossResults2, \"Average Loss for SGD\")\n",
    "create_heatmap(avergaeAccuracyResults, \"Average Accuracy for RPROP\")\n",
    "create_heatmap(averageAccuracyResults2, \"Average Accuracy for SGD\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
